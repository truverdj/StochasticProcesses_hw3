---
title: "STA 531 HW3"
author: "Daniel Truver"
date: "2/15/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### (1) Rejection Sampling of the Posterior

I doubt this is correct, but I've been staring at it for hours and can't think of anything else.

$$
\begin{aligned}
\pi(\theta\mid y) 
&= \frac{\pi_0(\theta)L(\theta)}{\int_{-\infty}^{\infty}\pi_0(\theta)L(\theta)d\theta}\\
&\leq \frac{\pi_0(\theta)\max_\theta L(\theta)}{\int_{-\infty}^{\infty}\pi_0(\theta)L(\theta)d\theta} \\
&= \frac{\pi_0(\theta)L(\theta_{MLE})}{\int_{-\infty}^{\infty}\pi_0(\theta)L(\theta)d\theta}
\end{aligned}
$$

Then we use rejection sampling on the unnormalized posterior. 

$$
\pi(\theta\mid y) \int_{-\infty}^{\infty}\pi_0(\theta)L(\theta)d\theta \leq \pi_0(\theta)L(\theta_{MLE})
$$

That is, the unnormalized density is bounded above by $$c = L(\theta_{MLE}) $$ times the posterior. Rejection sampling should still work since $\pi_0(\theta)$ is a bonafide probability distribution.

#### (2) Normal-Cauchy Bayes Estimator (MC Integration)
##### (a)
Techniques used for this question are based on "Introduction to Monte Carlo Statistical Methods with R" by Robert and Casella (2010).   
```{r plotIntegrands}
x = 0
topf = function(theta){  theta/(1+theta^2)*exp(-(x-theta)^2/2)}
botf = function(theta){  1/(1+theta^2)*exp(-(x-theta)^2/2)}
{
  plot(topf, -5, 5, ylim = c(-1,1), main = "Top in Black, Bottom in Red")
  plot(botf, -5, 5, add=TRUE, col = 2)
}
```  

```{r 2aCalculation}
nsim = 100000
set.seed(2018)
cauchySamples = rcauchy(nsim)
x = 0
I_0 = mean(cauchySamples*dnorm(cauchySamples, mean = x))/mean(dnorm(cauchySamples, mean = x))
cat("The estimate for x =",x, "is", round(I_0,4))
x = 1
I_1 = mean(cauchySamples*dnorm(cauchySamples, mean = x))/mean(dnorm(cauchySamples, mean = x))
cat("The estimate for x =",x, "is", round(I_1,4))
```

So, yeah, the variance, about that, there's this formula on page 126 of the Robert and Casella book. If we take $X\sim N(0,1),~ W\sim C(0,1)$, we estimate the variance to be $3/n$. 

...

Anyway, using this estimate, if we want the integral to be accurate to 3 digits with probability 0.95, we need the 95% confidence interval to remain steady in the third digit. We can do this by achieving

$$
1.96\cdot SE < 0.0001 \implies \sqrt{3/n} < 0.0001/1.96 \implies n > 1.15 \times 10^9
$$

Throwing this back into the code and taking a brief coffee break, we obtain
```{r 2b.nowWithMoreIterations}
nsim = 1.15e9
set.seed(2018)
cauchySamples = rcauchy(nsim)
x = 0
I_0 = mean(cauchySamples*dnorm(cauchySamples, mean = x))/mean(dnorm(cauchySamples, mean = x))
cat("The estimate for x =",x, "is", round(I_0,4), "and we are 95% confident about 3 decimal points.")
```

#### (3) Normal Integration 

##### (a)
I interpret this problem as "we know the answer to this question and we want to assess how long it takes the Monte Carlo indicator functions to reach 3 digits of accuracy."
```{r 3a.doingItByIndicators}
set.seed(2018)
truth = 1 - pnorm(2.5, mean = 0, sd = 1)
wegotit = FALSE
k = 4
while (!wegotit) {
  nsim = 10^k
  Z = rnorm(nsim)
  P = mean(Z > 2.5 | Z < 2.5)/2
  diff_P.truth = abs(P - truth)
  if (diff_P.truth <= 1e-4){
    wegotit = TRUE
    threshold = 10^k
  }
  k = k + 1
}
```

##### (b)

We will be using the uniform distribution again because of convenience. This time, we'll take $a = 5.3, b= 10$. The variance of our $\hat{\theta}$ will be $(10-5.3)^2/n$. If we want 3 digits of accuracy, we again need
$$
\sqrt{var(\hat\theta)} = \sqrt{\frac{4.7^2}{n}} < 0.0001 \implies n > 2.209 \times 10^9
$$
As for finding the .995 cutoff of the gamma(1,1), eh...

```{r mcIntegrationGamma}
set.seed(2018)
n = 2.209e9
a = 5.3; b = 10
alpha = 1; beta = 1
points = runif(n, a, b)
V = b-a
funPoints = dgamma(points, 1,1)
cat("We get that the estimate probability of X>5.3 is", V*mean(funPoints),
    "which is approximately 0.005.")
```

#### (4) Optimal Instrumental Distribution

Yeah, no. I haven't a clue. 