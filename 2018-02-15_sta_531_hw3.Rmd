---
title: "STA 531 HW3"
author: "Daniel Truver"
date: "2/15/2018"
header-includes:
  - \usepackage{soul}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### (1) Rejection Sampling of the Posterior

I doubt this is correct, but I've been staring at it for hours and can't think of anything else.

$$
\begin{aligned}
\pi(\theta\mid y) 
&= \frac{\pi_0(\theta)L(\theta)}{\int_{-\infty}^{\infty}\pi_0(\theta)L(\theta)d\theta}\\
&\leq \frac{\pi_0(\theta)\max_\theta L(\theta)}{\int_{-\infty}^{\infty}\pi_0(\theta)L(\theta)d\theta} \\
&= \frac{\pi_0(\theta)L(\theta_{MLE})}{\int_{-\infty}^{\infty}\pi_0(\theta)L(\theta)d\theta}
\end{aligned}
$$

Then we use rejection sampling on the unnormalized posterior. 

$$
\pi(\theta\mid y) \int_{-\infty}^{\infty}\pi_0(\theta)L(\theta)d\theta \leq \pi_0(\theta)L(\theta_{MLE})
$$

That is, the unnormalized density is bounded above by $$c = L(\theta_{MLE}) $$ times the posterior. Rejection sampling should still work since $\pi_0(\theta)$ is a bonafide probability distribution.

#### (2) Normal-Cauchy Bayes Estimator (MC Integration)

##### (a)


Techniques used for this question are based on "Introduction to Monte Carlo Statistical Methods with R" by Robert and Casella (2010).   
```{r plotIntegrands}
x = 0
topf = function(theta){  theta/(1+theta^2)*exp(-(x-theta)^2/2)}
botf = function(theta){  1/(1+theta^2)*exp(-(x-theta)^2/2)}
{
  plot(topf, -5, 5, ylim = c(-1,1), main = "Top in Black, Bottom in Red")
  plot(botf, -5, 5, add=TRUE, col = 2)
}
```  

```{r 2aCalculation}
nsim = 100000
set.seed(2018)
cauchySamples = rcauchy(nsim)
x = 0
I_0 = mean(cauchySamples*dnorm(cauchySamples, mean = x))/mean(dnorm(cauchySamples, mean = x))
cat("The estimate for x =",x, "is", round(I_0,4),
    "\n")
x = 1
I_1 = mean(cauchySamples*dnorm(cauchySamples, mean = x))/mean(dnorm(cauchySamples, mean = x))
cat("The estimate for x =",x, "is", round(I_1,4))
```

So, yeah, the variance, about that, there's this formula on page 126 of the Robert and Casella book. If we take $X\sim N(0,1),~ W\sim C(0,1)$, we estimate the variance to be $3/n$. 

...

Anyway, using this estimate, if we want the integral to be accurate to 3 digits with probability 0.95, we need the 95% confidence interval to remain steady in the third digit. We can do this by achieving

$$
1.96\cdot SE < 0.0001 \implies \sqrt{3/n} < 0.0001/1.96 \implies n > 1.15 \times 10^9
$$

This would kill my laptop, so let's trust the theory.
\newpage 

#### (3) Normal Integration 

##### (a)
I interpret this problem as "we know this probability and we want to assess how long it takes the Monte Carlo indicator functions to reach 3 digits of accuracy."
```{r 3a.doingItByIndicators}
set.seed(2018)
truth = 1 - pnorm(2.5, mean = 0, sd = 1)
wegotit = FALSE
k = 5
nsim = 10^k
Z = rnorm(nsim)
P = 1 - mean(Z <= 2.5)
cat("Out estiamte of P(Z > 2.5) is",round(P,4),".")
```

Using known results, we know the true probability we're looking for is `r round(pnorm(2.5),5)`. This means the exact variance of our esimator is 
$$
(0.99379)(1-0.99379)/n .
$$

So, if we want 3 digits of accuracy, we want the confidence interval to be invariant in the third decimal place. We can ensure this with 
$$
1.96\cdot\sqrt{(0.99379)(1-0.99379)/n } < 0.0001 \implies n > 2.37\times 10^6
$$  


##### (b)

The probability of $X > 5.3$ shouldn't be a problem. On the other hand, finding .995 cutoff of gamma(1,1), very iffy.

```{r}
set.seed(2018)
nsim = 1e6
X = rgamma(nsim, 1, 1)
P = mean(X > 5.3)
cat("Our estimated P(X>5.3) =", P, ".")
```

#### (4) In Class We Discussed the Optimal Instrumental Distribution

##### (a)
Let me just check the \st{Casella Book} class notes real quick.

We know that the optimal choice is 
$$
g^*(x) = \frac{|h(x)|f(x)}{\int_X |h(z)|f(z)~dz }
$$

If we insert this into our estimator, with $x_j \sim g$, 
$$
\frac{\sum_{j=1}^m h(x_j) f(x_j)/g(x_j)}{\sum_{j=1}^m f(x_j)/g(x_j)},
$$

then we get: 
$$
\begin{aligned}
\frac{\sum_{j=1}^m h(x_j) f(x_j)/g^*(x_j)}{\sum_{j=1}^m f(x_j)/g^*(x_j)} 
&= \frac{\sum_{j=1}^m h(x_j) f(x_j)/|h(x_j)|f(x_j)}{\sum_{j=1}^m f(x_j)/|h(x_j)|f(x_j)} \\
&= \frac{\sum_{j=1}^m h(x_j)|h(x_j)|^{-1}}{\sum_{j=1}^m |h(x_j)|^{-1}}
\end{aligned}
$$

Just as planned. 

##### (b) 

```{r 4b.simulation}
set.seed(2018)
nsim = 1e5
truth = pnorm(2) - pnorm(1)
Z = rnorm(nsim)
P.carlo = mean(1 < Z & Z < 2)
h = function(x){return(1 * (1 < x & x < 2))}
g.star = function(x){return(h(x) * dnorm(x,0,1)/truth)}
imp_summands = h(Z) * dnorm(Z) / g.star(Z)
P.opt = sum( imp_summands[!is.nan(imp_summands)] ) / sum(!is.nan(imp_summands))
```

```{r, echo=FALSE}
cat("The error of the standard MC estimator is", abs(P.carlo - truth), ".\n")
cat("The error of the Importance Sampler with the optimal choice of distribution is", 
    abs(P.opt - truth), ".\n")
```

This is expected, because, as stated in the text, the optimal distribution is really just a formality since it requires knowledge of the truth. 